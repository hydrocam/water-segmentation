{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqSxt9-x9HVA"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from numpy import zeros\n",
        "from numpy.random import randint\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "from statistics import mean\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "# Data Viz\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ed2veR-75OR"
      },
      "source": [
        "Import segment_anything and its dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUezUlIc7Qf_"
      },
      "outputs": [],
      "source": [
        "! pip install torch torchvision &> /dev/null\n",
        "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
        "\n",
        "# Download the ViT versions of the mobile SAM model weights from github repository.\n",
        "!git clone https://github.com/ChaoningZhang/MobileSAM.git\n",
        "%cd MobileSAM\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAzRwyB7qSF"
      },
      "source": [
        "Improt Training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QBoQoGy7SE_"
      },
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your training images and labels\n",
        "image_path = \"/path/to/train/images\"   # <-- Replace with actual image folder path\n",
        "label_path = \"/path/to/train/labels\"   # <-- Replace with actual label folder path\n",
        "\n",
        "# === Load Image Paths ===\n",
        "# Count total number of image files (e.g., .jpg format)\n",
        "all_image_paths = sorted(glob(os.path.join(image_path, \"*.jpg\")))  # Use .png if needed\n",
        "total_images = len(all_image_paths)\n",
        "print(f\"Total Number of Images: {total_images}\")\n",
        "\n",
        "# === Load Label Paths ===\n",
        "# Count total number of label files (e.g., .png format for segmentation masks)\n",
        "all_label_paths = sorted(glob(os.path.join(label_path, \"*.png\")))\n",
        "total_labels = len(all_label_paths)\n",
        "print(f\"Total Number of Labels: {total_labels}\")\n",
        "\n",
        "# === Match Images and Labels ===\n",
        "# Assuming both are in matching order and of equal length\n",
        "train_image_paths = all_image_paths[:total_images]\n",
        "train_label_paths = all_label_paths[:total_labels]\n",
        "\n",
        "# Preview label paths (for verification)\n",
        "print(\"Sample label paths:\")\n",
        "for path in train_label_paths[:5]:\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbJqU3G8796o"
      },
      "source": [
        "Improt Validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AWoraSf7crv"
      },
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your validation images and labels\n",
        "val_image_path = \"/path/to/valid/images\"   # <-- Replace with actual validation image folder\n",
        "val_label_path = \"/path/to/valid/labels\"   # <-- Replace with actual validation label folder\n",
        "\n",
        "# === Load Validation Image Paths ===\n",
        "# Collect and sort all .jpg image files in the validation folder\n",
        "val_all_image_paths = sorted(glob(os.path.join(val_image_path, \"*.jpg\")))\n",
        "val_total_images = len(val_all_image_paths)\n",
        "print(f\"Total Number of Validation Images: {val_total_images}\")\n",
        "\n",
        "# === Load Validation Label Paths ===\n",
        "# Collect and sort all .png label files in the validation folder\n",
        "val_all_label_paths = sorted(glob(os.path.join(val_label_path, \"*.png\")))\n",
        "val_total_labels = len(val_all_label_paths)\n",
        "print(f\"Total Number of Validation Labels: {val_total_labels}\")\n",
        "\n",
        "# === Match Images and Labels (by order) ===\n",
        "# This assumes one-to-one correspondence between image and label files\n",
        "Val1_image_paths = val_all_image_paths[:val_total_images]\n",
        "Val1_label_paths = val_all_label_paths[:val_total_labels]\n",
        "\n",
        "# Preview a few label paths to confirm loading\n",
        "print(\"Sample validation label paths:\")\n",
        "for path in Val1_label_paths[:5]:\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06JFMag8k85"
      },
      "source": [
        "Reading ground_truth_masks for training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfj5zhu2hBfn"
      },
      "outputs": [],
      "source": [
        "# Please dont run this line if you would like to use the original size of input images.\n",
        "desired_size=(640, 640)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0l_K6w8fuN"
      },
      "outputs": [],
      "source": [
        "# === Load and Process Ground Truth Masks ===\n",
        "# This dictionary will store binary masks where pixel > 0 is treated as True\n",
        "ground_truth_masks = {}\n",
        "\n",
        "for idx in range(len(train_label_paths)):\n",
        "    # Read the label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(train_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if desired_size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask (True where pixel > 0)\n",
        "    ground_truth_masks[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Optional: Print number of masks and preview a sample\n",
        "print(f\"Total ground truth masks loaded: {len(ground_truth_masks)}\")\n",
        "print(\"Example binary mask shape:\", ground_truth_masks[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhve4HL6hTjA"
      },
      "outputs": [],
      "source": [
        "# === Load and Process Validation Ground Truth Masks ===\n",
        "# This dictionary will store binary masks for validation data\n",
        "ground_truth_masksv = {}\n",
        "\n",
        "for idx in range(len(Val1_label_paths)):\n",
        "    # Read the validation label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(Val1_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if a desired size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask: True where pixel > 0\n",
        "    ground_truth_masksv[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Total validation ground truth masks loaded: {len(ground_truth_masksv)}\")\n",
        "print(\"Example validation mask shape:\", ground_truth_masksv[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSc9z-Ax8xPz"
      },
      "source": [
        "Import SAM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM0PLPQi9GKl"
      },
      "outputs": [],
      "source": [
        "model_type = \"vit_t\"\n",
        "checkpoint = \"mobile_sam.pt\"\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ryKa3Jh8z58"
      },
      "outputs": [],
      "source": [
        "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "import torch\n",
        "\n",
        "# === Load Pretrained SAM Model ===\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "\n",
        "# === Device Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam_model.to(device)\n",
        "\n",
        "# === Set to Training Mode/Inferencing Mode (for fine-tuning) ===\n",
        "sam_model.eval()\n",
        "sam_model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJAFTJ2L89hq"
      },
      "source": [
        "#Step 1: Preprocess the images for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3hOLw_ti7HM"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# === Initialize Resize Transformation ===\n",
        "# This ensures images and masks are resized while preserving aspect ratio\n",
        "transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "\n",
        "# === Containers for Processed Data ===\n",
        "transformed_data = defaultdict(dict)  # holds resized + preprocessed inputs for SAM\n",
        "ground_truth_masks = {}               # holds binary masks for training\n",
        "\n",
        "# === Preprocess All Training Samples ===\n",
        "for k in range(len(train_image_paths)):\n",
        "    # Load RGB image and grayscale ground truth mask\n",
        "    image_bgr = cv2.imread(train_image_paths[k])\n",
        "    mask_gray = cv2.imread(train_lable_paths[k], cv2.IMREAD_GRAYSCALE)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply resizing transformation to both image and mask\n",
        "    resized_image = transform.apply_image(image_rgb)\n",
        "    resized_mask = transform.apply_image(mask_gray)\n",
        "\n",
        "    # Convert image to torch tensor and format as [1, 3, H, W]\n",
        "    image_tensor = torch.as_tensor(resized_image, device=device).permute(2, 0, 1).unsqueeze(0).contiguous()\n",
        "\n",
        "    # Preprocess for SAM (normalization, padding, etc.)\n",
        "    input_tensor = sam_model.preprocess(image_tensor)\n",
        "\n",
        "    # Store processed image and metadata\n",
        "    transformed_data[k]['image'] = input_tensor\n",
        "    transformed_data[k]['input_size'] = resized_image.shape[:2][::-1]  # (W, H)\n",
        "    transformed_data[k]['original_image_size'] = image_rgb.shape[:2]   # (H, W)\n",
        "\n",
        "    # Store resized mask as binary (bool)\n",
        "    ground_truth_masks[k] = (resized_mask > 0)\n",
        "\n",
        "print(f\"Preprocessed {len(transformed_data)} training samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Initialize Resize Transformation ===\n",
        "transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "\n",
        "# === Storage for Results ===\n",
        "ground_truth_masksv = {}   # Ground truth validation masks (resized and binarized)\n",
        "images_tuned_list = {}     # Resized RGB images used for prediction\n",
        "masks_tuned_list = {}      # Predicted masks from SAM\n",
        "\n",
        "# === Loop Through Validation Images ===\n",
        "for s in range(len(Val1_image_paths)):\n",
        "    # Load RGB image and grayscale ground truth mask\n",
        "    image_bgr = cv2.imread(Val1_image_paths[s])\n",
        "    gt_mask = cv2.imread(Val1_lable_paths[s], cv2.IMREAD_GRAYSCALE)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize image and ground truth using SAM’s aspect-preserving transform\n",
        "    resized_image = transform.apply_image(image_rgb)\n",
        "    resized_mask = transform.apply_image(gt_mask)\n",
        "\n",
        "    # Set resized image for SAM prediction\n",
        "    predictor_tuned.set_image(resized_image)\n",
        "\n",
        "    # Predict masks (without prompt)\n",
        "    masks_tuned, _, _ = predictor_tuned.predict(\n",
        "        point_coords=None,\n",
        "        box=None,\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "    # Convert SAM prediction to binary mask\n",
        "    pred_mask = masks_tuned[0]\n",
        "    binary_mask = (pred_mask > 0).astype(np.float32)\n",
        "\n",
        "    # Store outputs\n",
        "    images_tuned_list[s] = resized_image\n",
        "    masks_tuned_list[s] = binary_mask\n",
        "    ground_truth_masksv[s] = (resized_mask > 0).astype(np.float32)\n",
        "\n",
        "print(f\"Predictions complete for {len(Val1_image_paths)} validation images.\")"
      ],
      "metadata": {
        "id": "Lg2DTk0U4GNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwAvecns9Q_a"
      },
      "source": [
        "\n",
        "# Set up the optimizer, and Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbCmWJxa9LAS"
      },
      "outputs": [],
      "source": [
        "# === Training Configuration ===\n",
        "keys = list(ground_truth_masks.keys())  # List of training sample indices\n",
        "batch_size = 32                         # Adjust based on available GPU memory\n",
        "num_epochs = 5                          # Total number of training epochs\n",
        "\n",
        "# === Parameters to Fine-Tune ===\n",
        "# You can choose to fine-tune all components or a subset\n",
        "params_to_optimize = (\n",
        "    list(sam_model.mask_decoder.parameters()) +        # Mask decoder (usually the main fine-tuning target)\n",
        "    list(sam_model.image_encoder.parameters()) +       # Optional: include image encoder\n",
        "    list(sam_model.prompt_encoder.parameters())        # Optional: include prompt encoder\n",
        ")\n",
        "\n",
        "# === Optimizer ===\n",
        "# Adam optimizer with low learning rate for stable fine-tuning\n",
        "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-4, weight_decay=0)\n",
        "\n",
        "# === Loss Function ===\n",
        "# Binary Cross-Entropy with logits is used for binary segmentation tasks\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtsntdLjK-Q"
      },
      "source": [
        "# Fine tuning SAM by Training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# === Validation DataLoader Setup ===\n",
        "# Here we're using a list of file paths as the dataset, which will later need to be wrapped in a proper Dataset class\n",
        "val_loader = DataLoader(Val1_image_paths, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# === Basic Validation Dataset Checks ===\n",
        "\n",
        "# Total number of validation examples\n",
        "num_val_examples = len(Val1_image_paths)\n",
        "print(f\"Number of validation examples: {num_val_examples}\")\n",
        "\n",
        "# Number of items returned by val_loader.dataset (same as above since it's a list)\n",
        "print(f\"Number of examples in validation dataset (via DataLoader): {len(val_loader.dataset)}\")\n",
        "\n",
        "# Number of batches in the validation DataLoader\n",
        "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
        "\n",
        "# === Safety Check ===\n",
        "# Prevent training from continuing if validation data is empty\n",
        "if num_val_examples == 0:\n",
        "    raise ValueError(\"The validation dataset is empty. Please check your data paths.\")"
      ],
      "metadata": {
        "id": "FksbJ8iC_bDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kqC-BCSbA8Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-sRk0gP9gbW"
      },
      "outputs": [],
      "source": [
        "# === Accuracy Calculation Function ===\n",
        "def calculate_accuracy(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary accuracy between predicted and ground truth masks.\n",
        "    \"\"\"\n",
        "    binary_predictions = (predictions > 0.5).float()\n",
        "    accuracy = (binary_predictions == targets).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "# === Batch Training Function ===\n",
        "def train_on_batch(keys, batch_start, batch_end):\n",
        "    \"\"\"\n",
        "    Trains the SAM mask decoder on a batch of images and masks.\n",
        "    Returns batch loss and accuracy.\n",
        "    \"\"\"\n",
        "    batch_losses = []\n",
        "    batch_accuracies = []\n",
        "\n",
        "    for k in keys[batch_start:batch_end]:\n",
        "        # === Get input data and metadata\n",
        "        input_image = transformed_data[k]['image'].to(device)\n",
        "        input_size = transformed_data[k]['input_size']\n",
        "        original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "        # === Forward Pass (frozen encoders)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = sam_model.image_encoder(input_image)\n",
        "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "                points=None, boxes=None, masks=None\n",
        "            )\n",
        "\n",
        "        low_res_masks, _ = sam_model.mask_decoder(\n",
        "            image_embeddings=image_embedding,\n",
        "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False\n",
        "        )\n",
        "\n",
        "        # === Resize prediction to original size\n",
        "        upscaled_masks = sam_model.postprocess_masks(\n",
        "            low_res_masks, input_size, original_image_size\n",
        "        ).to(device)\n",
        "\n",
        "        # === Resize ground truth mask to match output\n",
        "        gt_np = ground_truth_masks[k].astype(np.uint8)\n",
        "        resized_gt = cv2.resize(gt_np, upscaled_masks.shape[-2:][::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        gt_binary_mask = torch.tensor(resized_gt, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # === Compute loss and update weights\n",
        "        loss = loss_fn(upscaled_masks, gt_binary_mask)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # === Record metrics\n",
        "        batch_losses.append(loss.item())\n",
        "        batch_accuracies.append(calculate_accuracy(torch.sigmoid(upscaled_masks), gt_binary_mask))\n",
        "\n",
        "    return batch_losses, batch_accuracies\n",
        "\n",
        "# === Epoch Training Loop ===\n",
        "losses, accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    print(f\"\\n--- EPOCH {epoch + 1}/{num_epochs} ---\")\n",
        "\n",
        "    for batch_start in range(0, len(keys), batch_size):\n",
        "        batch_end = min(batch_start + batch_size, len(keys))\n",
        "        batch_losses, batch_accuracies = train_on_batch(keys, batch_start, batch_end)\n",
        "\n",
        "        batch_loss = mean(batch_losses)\n",
        "        batch_accuracy = mean(batch_accuracies)\n",
        "        epoch_losses.append(batch_loss)\n",
        "        epoch_accuracies.extend(batch_accuracies)\n",
        "\n",
        "        print(f'Batch [{batch_start + 1}–{batch_end}] | Loss: {batch_loss:.6f} | Accuracy: {batch_accuracy:.4f}')\n",
        "\n",
        "    # === End of Epoch ===\n",
        "    mean_train_loss = mean(epoch_losses)\n",
        "    mean_train_accuracy = mean(epoch_accuracies)\n",
        "    losses.append(mean_train_loss)\n",
        "    accuracies.append(mean_train_accuracy)\n",
        "\n",
        "    print(f'\\nEpoch {epoch + 1} Summary:')\n",
        "    print(f'➤ Mean Training Loss: {mean_train_loss:.6f}')\n",
        "    print(f'➤ Mean Training Accuracy: {mean_train_accuracy:.4f}')\n",
        "\n",
        "    # Clear cache to manage memory\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ground_truth_masksv))\n",
        "print(ground_truth_masksv.keys() if isinstance(ground_truth_masksv, dict) else len(ground_truth_masksv))\n"
      ],
      "metadata": {
        "id": "dvzbeYATCd7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mixgUB8H-sTs"
      },
      "source": [
        "#Step 3: Testing fine-tuned SAM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzfJwH6E-tHr"
      },
      "outputs": [],
      "source": [
        "# Set the paths to your test images and labels\n",
        "test_image_dir = \"/path/to/test/images\"   # <-- Update this path\n",
        "test_label_dir = \"/path/to/test/labels\"   # <-- Update this path\n",
        "\n",
        "# === Load and Sort Test Image Paths ===\n",
        "# Collect all test image files (e.g., .jpg)\n",
        "all_test_image_paths = sorted(glob(os.path.join(test_image_dir, \"*.jpg\")))\n",
        "test_total_images = len(all_test_image_paths)\n",
        "print(f\"Total Number of Test Images: {test_total_images}\")\n",
        "\n",
        "# === Load and Sort Test Label Paths ===\n",
        "# Collect all test label files (e.g., .png masks)\n",
        "all_test_label_paths = sorted(glob(os.path.join(test_label_dir, \"*.png\")))\n",
        "test_total_labels = len(all_test_label_paths)\n",
        "print(f\"Total Number of Test Labels: {test_total_labels}\")# === Count test samples ===\n",
        "test_total_images = len(Test_image_paths)\n",
        "test_total_labels = len(Test_lable_paths)\n",
        "\n",
        "# === Dictionary to store binary test masks ===\n",
        "ground_truth_test_masks = {}\n",
        "\n",
        "# === Load and resize ground truth masks ===\n",
        "for k in range(test_total_labels):\n",
        "    # Read mask as single-channel grayscale\n",
        "    gt_gray = cv2.imread(Test_lable_paths[k], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize if desired size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_gray = cv2.resize(gt_gray, desired_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Convert to binary mask (float32 format)\n",
        "    ground_truth_test_masks[k] = (gt_gray > 0).astype(np.float32)\n",
        "\n",
        "print(f\" Loaded {len(ground_truth_test_masks)} ground truth test masks.\")\n",
        "\n",
        "# === Match Image and Label Paths ===\n",
        "# These lists can now be used for DataLoader or evaluation\n",
        "Test_image_paths = all_test_image_paths[:test_total_images]\n",
        "Test_label_paths = all_test_label_paths[:test_total_labels]\n",
        "\n",
        "# Optional: Print a few samples to verify\n",
        "print(\"Sample test image path:\", Test_image_paths[0] if Test_image_paths else \"No images found\")\n",
        "print(\"Sample test label path:\", Test_label_paths[0] if Test_label_paths else \"No labels found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1vSEi8M_BHV"
      },
      "source": [
        "Box for Test *data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJZ7fH_m_Mc4"
      },
      "source": [
        "Ground_truth_test_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbvYphfc_IC9"
      },
      "outputs": [],
      "source": [
        "# Dictionary to hold ground truth binary masks for test data\n",
        "ground_truth_test_masks = {}\n",
        "\n",
        "# === Load and Process Each Test Mask ===\n",
        "for idx in range(len(Test_label_paths)):\n",
        "    # Read label image in color (3-channel); expected mask is in the red channel\n",
        "    gt_color = cv2.imread(Test_label_paths[idx])\n",
        "\n",
        "    # Extract the red channel only and convert to binary mask\n",
        "    # Note: OpenCV loads in BGR, so red is at index 2\n",
        "    binary_mask = (gt_color[:, :, 2] > 0).astype(np.float32)\n",
        "\n",
        "    # Resize if specified\n",
        "    if desired_size is not None:\n",
        "        binary_mask = cv2.resize(binary_mask, desired_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Store in dictionary\n",
        "    ground_truth_test_masks[idx] = binary_mask\n",
        "\n",
        "print(f\"Loaded {len(ground_truth_test_masks)} ground truth test masks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwtDqj8z_hUa"
      },
      "source": [
        "Load the trained model with best parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o5BRibjAII3"
      },
      "source": [
        "Prediction using Fine_tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjw_sGIk_05Z"
      },
      "outputs": [],
      "source": [
        "# === Output containers ===\n",
        "images_tuned_list = {}  # Stores resized RGB input images\n",
        "masks_tuned_list = {}   # Stores binary segmentation masks\n",
        "\n",
        "# === Run inference on all test images ===\n",
        "for k in range(test_total_images):\n",
        "    # Load image from disk\n",
        "    image_bgr = cv2.imread(Test_image_paths[k])  # use Test_image_paths here\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Optional resizing\n",
        "    if desired_size is not None:\n",
        "        image_rgb = cv2.resize(image_rgb, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Set image for SAM predictor\n",
        "    predictor_tuned.set_image(image_rgb)\n",
        "\n",
        "    # Run prediction (no prompt input)\n",
        "    masks_tuned, _, _ = predictor_tuned.predict(\n",
        "        point_coords=None,\n",
        "        box=None,\n",
        "        multimask_output=False  # Only return most confident mask\n",
        "    )\n",
        "\n",
        "    # Convert first predicted mask to binary\n",
        "    pred_mask = masks_tuned[0]  # shape: (H, W)\n",
        "    binary_mask = (pred_mask > 0).astype(np.float32)\n",
        "\n",
        "    # Store results\n",
        "    images_tuned_list[k] = image_rgb\n",
        "    masks_tuned_list[k] = binary_mask\n",
        "\n",
        "print(f\"Inference complete on {test_total_images} test images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJS2gydcAKvB"
      },
      "source": [
        "Plot results on all of the Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdIvmRZ7_-Yc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# === Configuration ===\n",
        "n_images = len(images_tuned_list)\n",
        "n_cols = 4  # Images per row\n",
        "n_rows = (n_images + n_cols - 1) // n_cols  # Automatically calculate needed rows\n",
        "\n",
        "# === Create subplot grid ===\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "\n",
        "# Ensure axs is always 2D (e.g., when n_rows = 1)\n",
        "axs = np.atleast_2d(axs)\n",
        "\n",
        "# === Loop through and display each image + overlay ===\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        index = i * n_cols + j\n",
        "        ax = axs[i, j]\n",
        "\n",
        "        if index < n_images:\n",
        "            # Load image and binary mask\n",
        "            image = images_tuned_list[index]\n",
        "            mask = masks_tuned_list[index]\n",
        "\n",
        "            # Display the RGB image\n",
        "            ax.imshow(image, interpolation='none')\n",
        "\n",
        "            # Create blue overlay for mask\n",
        "            blue_overlay = np.stack([\n",
        "                np.zeros_like(mask),\n",
        "                np.zeros_like(mask),\n",
        "                (mask > 0).astype(np.float32)\n",
        "            ], axis=-1)\n",
        "\n",
        "            # Overlay with transparency\n",
        "            ax.imshow(blue_overlay, alpha=0.5)\n",
        "\n",
        "        # Turn off axes for visual cleanliness\n",
        "        ax.axis('off')\n",
        "\n",
        "# === Adjust layout ===\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo2DTq0m3m9D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# === Binary Metrics for One Prediction ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary segmentation metrics for a single predicted mask vs ground truth.\n",
        "    Inputs:\n",
        "        predictions (numpy array): predicted mask, float32, range [0,1] or binary\n",
        "        targets (numpy array): ground truth mask, binary (0 or 1)\n",
        "    Returns:\n",
        "        Tuple of metrics: accuracy, precision, recall, F1-score, IoU, kappa, FP, FN, TP, TN, dice\n",
        "    \"\"\"\n",
        "    # Flatten and convert to binary\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    # Confusion matrix components\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    # Metrics with small epsilon to avoid division by zero\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    # Cohen’s kappa\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Average Metrics Across Dataset ===\n",
        "def calculate_average_metrics(predictions_list, targets_list):\n",
        "    \"\"\"\n",
        "    Computes average binary segmentation metrics across a dataset.\n",
        "    Inputs:\n",
        "        predictions_list: dictionary or list of predicted masks\n",
        "        targets_list: dictionary or list of ground truth masks\n",
        "    Returns:\n",
        "        Dictionary of averaged metrics\n",
        "    \"\"\"\n",
        "    num_masks = len(predictions_list)\n",
        "\n",
        "    total_metrics = {\n",
        "        'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f_score': 0.0,\n",
        "        'iou': 0.0, 'kappa': 0.0, 'FP': 0, 'FN': 0, 'MAR': 0.0, 'FAR': 0.0, 'dice': 0.0\n",
        "    }\n",
        "\n",
        "    for i in range(num_masks):\n",
        "        pred = predictions_list[i]\n",
        "        gt = targets_list[i]\n",
        "        metrics = binary_segmentation_metrics(pred, gt)\n",
        "\n",
        "        # Accumulate each metric\n",
        "        for metric_name, value in zip(total_metrics.keys(), metrics):\n",
        "            total_metrics[metric_name] += value\n",
        "\n",
        "        # Add False Negative Rate (Missed Alarm Rate, MAR) and False Alarm Rate (FAR)\n",
        "        TP, TN, FP, FN = metrics[8], metrics[9], metrics[6], metrics[7]\n",
        "        total_metrics['MAR'] += FN / (FN + TP + 1e-5)\n",
        "        total_metrics['FAR'] += FP / (FP + TN + 1e-5)\n",
        "\n",
        "    # Compute mean for each metric\n",
        "    avg_metrics = {k: v / num_masks for k, v in total_metrics.items()}\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# === Example Usage ===\n",
        "# Evaluate the SAM predictions vs. ground truth test masks\n",
        "avg_metrics = calculate_average_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Average Metrics on Test Set ===\")\n",
        "for metric_name, value in avg_metrics.items():\n",
        "    print(f\"{metric_name.upper():<8}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-jxoBhi0YA4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from skimage.color import rgb2gray\n",
        "import math\n",
        "\n",
        "# === Configuration ===\n",
        "border_color = 'black'\n",
        "border_width = 0.35\n",
        "alpha_gray = 0.5  # transparency for base grayscale image\n",
        "alpha_tp = 0.5    # transparency for True Positives (blue)\n",
        "alpha_fa = 0.6    # False Alarms (green)\n",
        "alpha_ma = 0.6    # Missed Alarms (magenta)\n",
        "\n",
        "# === Determine Grid Layout ===\n",
        "num_images = len(images_tuned_list)\n",
        "cols = 4\n",
        "rows = math.ceil(num_images / cols)\n",
        "\n",
        "# === Setup Plot ===\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
        "axs = axs.flatten()  # flatten 2D axes array for 1D indexing\n",
        "\n",
        "# === Choose Which Images to Display (all by default) ===\n",
        "selected_indices = list(range(num_images))\n",
        "\n",
        "# === Visualization Loop ===\n",
        "for i, index in enumerate(selected_indices):\n",
        "    if index >= num_images:\n",
        "        break\n",
        "\n",
        "    # Convert image to grayscale\n",
        "    gray_image = rgb2gray(images_tuned_list[index])\n",
        "\n",
        "    # Get predicted and ground truth masks\n",
        "    pred_mask = masks_tuned_list[index]\n",
        "    gt_mask = ground_truth_test_masks[index]\n",
        "\n",
        "    ax = axs[i]\n",
        "    ax.imshow(gray_image, cmap='gray', interpolation='none', alpha=alpha_gray)\n",
        "\n",
        "    # === Mask Overlays ===\n",
        "\n",
        "    # True Positives: prediction and ground truth both 1 (blue)\n",
        "    tp_mask = (pred_mask == 1) & (gt_mask == 1)\n",
        "    tp_rgba = np.stack([np.zeros_like(tp_mask), np.zeros_like(tp_mask), tp_mask], axis=-1)\n",
        "    tp_rgba = np.concatenate([tp_rgba, tp_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(tp_rgba, alpha=alpha_tp)\n",
        "\n",
        "    # False Alarms: prediction is 1, ground truth is 0 (green)\n",
        "    fa_mask = (pred_mask == 1) & (gt_mask == 0)\n",
        "    fa_rgba = np.stack([np.zeros_like(fa_mask), fa_mask, np.zeros_like(fa_mask)], axis=-1)\n",
        "    fa_rgba = np.concatenate([fa_rgba, fa_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(fa_rgba, alpha=alpha_fa)\n",
        "\n",
        "    # Missed Alarms: prediction is 0, ground truth is 1 (magenta)\n",
        "    ma_mask = (pred_mask == 0) & (gt_mask == 1)\n",
        "    ma_rgba = np.stack([ma_mask, np.zeros_like(ma_mask), ma_mask], axis=-1)\n",
        "    ma_rgba = np.concatenate([ma_rgba, ma_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(ma_rgba, alpha=alpha_ma)\n",
        "\n",
        "    # Add black border\n",
        "    ax.add_patch(Rectangle((0, 0), gray_image.shape[1], gray_image.shape[0],\n",
        "                           linewidth=border_width, edgecolor=border_color, facecolor='none'))\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "# === Turn Off Any Unused Axes ===\n",
        "for j in range(len(selected_indices), len(axs)):\n",
        "    axs[j].axis('off')\n",
        "\n",
        "# === Save and Show ===\n",
        "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
        "output_path = \"./ViTSAM_Evaluation_Overlay.png\"  # <-- Update path as needed\n",
        "plt.savefig(output_path, dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Visualization saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcx89MdyAaAv"
      },
      "source": [
        "Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbLKpkbrYt8M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# === Metric Computation Function ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Categorization Function ===\n",
        "def categorize_metric(value, metric):\n",
        "    categories = {\n",
        "        'IoU': [(0.90, 'Excellent'), (0.85, 'Good'), (0.75, 'Fair'), (0.65, 'Poor'), (0, 'Unacceptable')],\n",
        "        'Precision': [(0.95, 'Excellent'), (0.85, 'Good'), (0.65, 'Moderate'), (0, 'Fail')],\n",
        "        'Kappa': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')],\n",
        "        'F-Score': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')],\n",
        "        'Recall': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')]\n",
        "    }\n",
        "\n",
        "    if metric in categories:\n",
        "        for threshold, label in categories[metric]:\n",
        "            if value >= threshold:\n",
        "                return label\n",
        "    return \"Unknown\"\n",
        "\n",
        "# === Compute Metrics for All Images ===\n",
        "def compute_all_metrics(predictions_list, targets_list):\n",
        "    all_metrics = []\n",
        "    for i in range(len(predictions_list)):\n",
        "        metrics = binary_segmentation_metrics(predictions_list[i], targets_list[i])\n",
        "        metric_names = ['Accuracy', 'Precision', 'Recall', 'F-Score', 'IoU', 'Kappa', 'FP', 'FN', 'TP', 'TN', 'Dice']\n",
        "        metric_dict = dict(zip(metric_names, metrics))\n",
        "\n",
        "        # Add categorized versions\n",
        "        for name in ['IoU', 'Precision', 'Kappa', 'F-Score', 'Recall']:\n",
        "            metric_dict[f'{name}_Category'] = categorize_metric(metric_dict[name], name)\n",
        "\n",
        "        all_metrics.append(metric_dict)\n",
        "    return all_metrics\n",
        "\n",
        "# === Summarize Counts and Percentages ===\n",
        "def summarize_category_counts(metrics_list, metric_name, labels):\n",
        "    counts = {label: 0 for label in labels}\n",
        "    total = len(metrics_list)\n",
        "\n",
        "    for m in metrics_list:\n",
        "        category = m.get(f\"{metric_name}_Category\")\n",
        "        if category in counts:\n",
        "            counts[category] += 1\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Counts and Percentages of Images in Each Category for {metric_name}:\")\n",
        "    for label in labels:\n",
        "        count = counts[label]\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"{label}: {count} ({percentage:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "# === Run All ===\n",
        "metrics_list = compute_all_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "summarize_category_counts(metrics_list, 'IoU',        ['Excellent', 'Good', 'Fair', 'Poor', 'Unacceptable'])\n",
        "summarize_category_counts(metrics_list, 'Precision',  ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'Kappa',      ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'F-Score',    ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'Recall',     ['Excellent', 'Good', 'Moderate', 'Fail'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}