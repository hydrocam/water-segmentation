{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqSxt9-x9HVA"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from numpy import zeros\n",
        "from numpy.random import randint\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "from statistics import mean\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "# Data Viz\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ed2veR-75OR"
      },
      "source": [
        "Import segment_anything and its dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUezUlIc7Qf_"
      },
      "outputs": [],
      "source": [
        "! pip install torch torchvision &> /dev/null\n",
        "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
        "\n",
        "# ==== Download Pretrained SAM Model Weights ====\n",
        "# Download the ViT versions of the SAM model weights from Facebook's public storage.\n",
        "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth &> /dev/null\n",
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAzRwyB7qSF"
      },
      "source": [
        "Improt Training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QBoQoGy7SE_"
      },
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your training images and labels\n",
        "image_path = \"/path/to/train/images\"   # <-- Replace with actual image folder path\n",
        "label_path = \"/path/to/train/labels\"   # <-- Replace with actual label folder path\n",
        "\n",
        "# === Load Image Paths ===\n",
        "# Count total number of image files (e.g., .jpg format)\n",
        "all_image_paths = sorted(glob(os.path.join(image_path, \"*.jpg\")))  # Use .png if needed\n",
        "total_images = len(all_image_paths)\n",
        "print(f\"Total Number of Images: {total_images}\")\n",
        "\n",
        "# === Load Label Paths ===\n",
        "# Count total number of label files (e.g., .png format for segmentation masks)\n",
        "all_label_paths = sorted(glob(os.path.join(label_path, \"*.png\")))\n",
        "total_labels = len(all_label_paths)\n",
        "print(f\"Total Number of Labels: {total_labels}\")\n",
        "\n",
        "# === Match Images and Labels ===\n",
        "# Assuming both are in matching order and of equal length\n",
        "train_image_paths = all_image_paths[:total_images]\n",
        "train_label_paths = all_label_paths[:total_labels]\n",
        "\n",
        "# Preview label paths (for verification)\n",
        "print(\"Sample label paths:\")\n",
        "for path in train_label_paths[:5]:\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbJqU3G8796o"
      },
      "source": [
        "Improt Validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AWoraSf7crv"
      },
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "# Set the path to your validation images and labels\n",
        "val_image_path = \"/path/to/valid/images\"   # <-- Replace with actual validation image folder\n",
        "val_label_path = \"/path/to/valid/labels\"   # <-- Replace with actual validation label folder\n",
        "\n",
        "# === Load Validation Image Paths ===\n",
        "# Collect and sort all .jpg image files in the validation folder\n",
        "val_all_image_paths = sorted(glob(os.path.join(val_image_path, \"*.jpg\")))\n",
        "val_total_images = len(val_all_image_paths)\n",
        "print(f\"Total Number of Validation Images: {val_total_images}\")\n",
        "\n",
        "# === Load Validation Label Paths ===\n",
        "# Collect and sort all .png label files in the validation folder\n",
        "val_all_label_paths = sorted(glob(os.path.join(val_label_path, \"*.png\")))\n",
        "val_total_labels = len(val_all_label_paths)\n",
        "print(f\"Total Number of Validation Labels: {val_total_labels}\")\n",
        "\n",
        "# === Match Images and Labels (by order) ===\n",
        "# This assumes one-to-one correspondence between image and label files\n",
        "Val1_image_paths = val_all_image_paths[:val_total_images]\n",
        "Val1_label_paths = val_all_label_paths[:val_total_labels]\n",
        "\n",
        "# Preview a few label paths to confirm loading\n",
        "print(\"Sample validation label paths:\")\n",
        "for path in Val1_label_paths[:5]:\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06JFMag8k85"
      },
      "source": [
        "Reading ground_truth_masks for training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfj5zhu2hBfn"
      },
      "outputs": [],
      "source": [
        "# Please dont run this line if you would like to use the original size of input images.\n",
        "desired_size=(640, 640)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0l_K6w8fuN"
      },
      "outputs": [],
      "source": [
        "# === Load and Process Ground Truth Masks ===\n",
        "# This dictionary will store binary masks where pixel > 0 is treated as True\n",
        "ground_truth_masks = {}\n",
        "\n",
        "for idx in range(len(train_label_paths)):\n",
        "    # Read the label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(train_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if desired_size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask (True where pixel > 0)\n",
        "    ground_truth_masks[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Optional: Print number of masks and preview a sample\n",
        "print(f\"Total ground truth masks loaded: {len(ground_truth_masks)}\")\n",
        "print(\"Example binary mask shape:\", ground_truth_masks[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhve4HL6hTjA"
      },
      "outputs": [],
      "source": [
        "# === Load and Process Validation Ground Truth Masks ===\n",
        "# This dictionary will store binary masks for validation data\n",
        "ground_truth_masksv = {}\n",
        "\n",
        "for idx in range(len(Val1_label_paths)):\n",
        "    # Read the validation label mask in grayscale\n",
        "    gt_grayscale = cv2.imread(Val1_label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Resize the mask if a desired size is specified\n",
        "    if desired_size is not None:\n",
        "        gt_grayscale = cv2.resize(gt_grayscale, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to binary mask: True where pixel > 0\n",
        "    ground_truth_masksv[idx] = (gt_grayscale > 0)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Total validation ground truth masks loaded: {len(ground_truth_masksv)}\")\n",
        "print(\"Example validation mask shape:\", ground_truth_masksv[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSc9z-Ax8xPz"
      },
      "source": [
        "Import SAM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHMTQFTa8ulE"
      },
      "outputs": [],
      "source": [
        "model_type = 'vit_b'\n",
        "checkpoint = 'sam_vit_b_01ec64.pth'\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrndFxRxIII9"
      },
      "outputs": [],
      "source": [
        "model_type = 'vit_l'\n",
        "checkpoint = 'sam_vit_l_0b3195.pth'\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM0PLPQi9GKl"
      },
      "outputs": [],
      "source": [
        "model_type = \"vit_h\"\n",
        "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "device = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ryKa3Jh8z58"
      },
      "outputs": [],
      "source": [
        "# === Import Required SAM Modules ===\n",
        "# Make sure the Segment Anything (SAM) package is installed and accessible\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "import torch\n",
        "\n",
        "# === Configuration ===\n",
        "# Set the model type: \"vit_b\", \"vit_l\", or \"vit_h\" depending on your .pth file\n",
        "model_type = \"vit_b\"  # or \"vit_l\", \"vit_h\", etc.\n",
        "\n",
        "# Path to the pretrained SAM checkpoint file (.pth)\n",
        "checkpoint = \"/path/to/sam_vit_b_01ec64.pth\"  # <-- Update with actual path\n",
        "\n",
        "# === Load Model ===\n",
        "# Use the model registry to initialize the correct SAM architecture\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "\n",
        "# Move model to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "sam_model.to(device)\n",
        "\n",
        "# === Set Model to Training Mode ===\n",
        "# Use `model.train()` when fine-tuning or training the model\n",
        "# For inference, use `model.eval()` instead\n",
        "sam_model.train()\n",
        "\n",
        "print(f\"SAM model ({model_type}) loaded on {device} and set to training mode.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJAFTJ2L89hq"
      },
      "source": [
        "#Step 1: Preprocess the images for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3hOLw_ti7HM"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "\n",
        "# Preprocessed image data will be stored in this dictionary\n",
        "transformed_data = defaultdict(dict)\n",
        "\n",
        "# Transformer that resizes image while preserving aspect ratio\n",
        "resize_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "\n",
        "# === Image Preprocessing Loop ===\n",
        "for idx in range(len(train_image_paths)):\n",
        "    # Load image from path\n",
        "    image = cv2.imread(train_image_paths[idx])\n",
        "\n",
        "    # Resize if a fixed input size is specified (e.g., for training consistency)\n",
        "    if desired_size is not None:\n",
        "        image = cv2.resize(image, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert BGR (OpenCV default) to RGB (SAM model expects RGB)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply SAM’s resizing transformation to match its input constraints\n",
        "    input_image_np = resize_transform.apply_image(image_rgb)\n",
        "\n",
        "    # Convert NumPy array to torch tensor and add batch dimension\n",
        "    input_image_tensor = torch.as_tensor(input_image_np, device=device)\n",
        "    input_image_tensor = input_image_tensor.permute(2, 0, 1).contiguous()[None, :, :, :]  # Shape: [1, 3, H, W]\n",
        "\n",
        "    # Preprocess using SAM model’s preprocessing method (normalization, padding, etc.)\n",
        "    input_tensor = sam_model.preprocess(input_image_tensor)\n",
        "\n",
        "    # Store processed data\n",
        "    transformed_data[idx]['image'] = input_tensor                          # Preprocessed image tensor\n",
        "    transformed_data[idx]['input_size'] = input_image_tensor.shape[-2:]   # Input tensor size (H, W)\n",
        "    transformed_data[idx]['original_image_size'] = image_rgb.shape[:2]    # Original image size (H, W)\n",
        "\n",
        "print(f\"Processed {len(transformed_data)} training images for SAM input.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwAvecns9Q_a"
      },
      "source": [
        "\n",
        "# Set up the optimizer, and Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbCmWJxa9LAS"
      },
      "outputs": [],
      "source": [
        "# === Training Hyperparameters ===\n",
        "lr = 1e-5                     # Learning rate for optimizer\n",
        "wd = 0                        # Weight decay (L2 regularization)\n",
        "batch_size = 32              # Number of samples per batch\n",
        "num_epochs = 5               # Total number of training epochs\n",
        "\n",
        "# === Optimizer Setup ===\n",
        "# Only the mask decoder parameters are being fine-tuned (others are frozen)\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# === Loss Function ===\n",
        "# Binary Cross Entropy with logits is commonly used for binary segmentation\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# === Device Setup ===\n",
        "# Automatically use GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Ground Truth Mask Keys ===\n",
        "# These lists are used to index into your ground truth dictionaries\n",
        "keys_train = list(ground_truth_masks.keys())\n",
        "keys_valid = list(ground_truth_masksv.keys())\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Training on {len(keys_train)} images, validating on {len(keys_valid)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtsntdLjK-Q"
      },
      "source": [
        "# Fine tuning SAM by Training data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# === Validation DataLoader Setup ===\n",
        "# Here we're using a list of file paths as the dataset, which will later need to be wrapped in a proper Dataset class\n",
        "val_loader = DataLoader(Val1_image_paths, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# === Basic Validation Dataset Checks ===\n",
        "\n",
        "# Total number of validation examples\n",
        "num_val_examples = len(Val1_image_paths)\n",
        "print(f\"Number of validation examples: {num_val_examples}\")\n",
        "\n",
        "# Number of items returned by val_loader.dataset (same as above since it's a list)\n",
        "print(f\"Number of examples in validation dataset (via DataLoader): {len(val_loader.dataset)}\")\n",
        "\n",
        "# Number of batches in the validation DataLoader\n",
        "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
        "\n",
        "# === Safety Check ===\n",
        "# Prevent training from continuing if validation data is empty\n",
        "if num_val_examples == 0:\n",
        "    raise ValueError(\"The validation dataset is empty. Please check your data paths.\")"
      ],
      "metadata": {
        "id": "FksbJ8iC_bDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kqC-BCSbA8Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-sRk0gP9gbW"
      },
      "outputs": [],
      "source": [
        "# === Utility: Accuracy Calculation ===\n",
        "def calculate_accuracy(predictions, targets):\n",
        "    binary_predictions = (predictions > 0.5).float()\n",
        "    accuracy = (binary_predictions == targets).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "# === Utility: One Batch Training ===\n",
        "def train_on_batch(keys, batch_start, batch_end):\n",
        "    batch_losses = []\n",
        "    batch_accuracies = []\n",
        "\n",
        "    for k in keys[batch_start:batch_end]:\n",
        "        input_image = transformed_data[k]['image'].to(device)\n",
        "        input_size = transformed_data[k]['input_size']\n",
        "        original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "        # Freeze encoder and prompt embeddings during training\n",
        "        with torch.no_grad():\n",
        "            image_embedding = sam_model.image_encoder(input_image)\n",
        "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(points=None, boxes=None, masks=None)\n",
        "\n",
        "        # Forward pass through mask decoder\n",
        "        low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
        "            image_embeddings=image_embedding,\n",
        "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        # Resize decoder output to original image size\n",
        "        upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
        "\n",
        "        # Convert predicted mask to binary format\n",
        "        binary_mask = (torch.sigmoid(upscaled_masks) > 0.5).float()\n",
        "\n",
        "        # Load and reshape ground truth mask\n",
        "        gt_mask = ground_truth_masks[k]\n",
        "        gt_tensor = torch.from_numpy(gt_mask.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "        # Loss + Backprop\n",
        "        loss = loss_fn(binary_mask, gt_tensor)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.item())\n",
        "        batch_accuracies.append(calculate_accuracy(binary_mask, gt_tensor))\n",
        "\n",
        "    return batch_losses, batch_accuracies\n",
        "\n",
        "# === Training Configuration ===\n",
        "losses, val_losses = [], []\n",
        "accuracies, val_acc = [], []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "    # === Training ===\n",
        "    for batch_start in range(0, len(keys), batch_size):\n",
        "        batch_end = min(batch_start + batch_size, len(keys))\n",
        "        batch_losses, batch_accuracies = train_on_batch(keys, batch_start, batch_end)\n",
        "\n",
        "        # Metrics\n",
        "        epoch_losses.append(mean(batch_losses))\n",
        "        epoch_accuracies.extend(batch_accuracies)\n",
        "\n",
        "        print(f'Batch: [{batch_start}-{batch_end}]  Loss: {mean(batch_losses):.4f}  Accuracy: {mean(batch_accuracies):.4f}')\n",
        "\n",
        "    mean_train_loss = mean(epoch_losses)\n",
        "    mean_train_accuracy = mean(epoch_accuracies)\n",
        "    losses.append(mean_train_loss)\n",
        "    accuracies.append(mean_train_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {mean_train_loss:.4f}, Accuracy: {mean_train_accuracy:.4f}\")\n",
        "\n",
        "    # === Validation ===\n",
        "    predictor_tuned = SamPredictor(sam_model)\n",
        "    val_loss, val_accuracy = 0.0, 0.0\n",
        "    num_val_examples = len(Val1_image_paths)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for s in range(num_val_examples):\n",
        "            image = cv2.imread(Val1_image_paths[s])\n",
        "            if desired_size is not None:\n",
        "                image = cv2.resize(image, desired_size)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Inference using tuned model\n",
        "            predictor_tuned.set_image(image)\n",
        "            masks_tuned, _, _ = predictor_tuned.predict(\n",
        "                point_coords=None,\n",
        "                box=None,\n",
        "                multimask_output=False,\n",
        "            )\n",
        "\n",
        "            pred_mask = torch.as_tensor((masks_tuned > 0)).float().unsqueeze(0).to(device)\n",
        "            gt_val_mask = torch.from_numpy(ground_truth_masksv[s].astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "            val_loss += loss_fn(pred_mask, gt_val_mask).item()\n",
        "            val_accuracy += calculate_accuracy(pred_mask, gt_val_mask)\n",
        "\n",
        "    val_loss /= num_val_examples\n",
        "    val_acc_epoch = val_accuracy / num_val_examples\n",
        "    val_losses.append(val_loss)\n",
        "    val_acc.append(val_acc_epoch)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc_epoch:.4f}\")\n",
        "\n",
        "# === Save Best Model Checkpoint ===\n",
        "# Save model only if validation loss improves\n",
        "if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "\n",
        "    # Define a general path to save the model\n",
        "    model_dir = \"./checkpoints\"  # <-- Change this to any directory you want\n",
        "    model_name = f\"best_model_epoch{epoch+1}.pth\"  # Include epoch or keep static if preferred\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save the model's state dictionary\n",
        "    save_path = os.path.join(model_dir, model_name)\n",
        "    torch.save(sam_model.state_dict(), save_path)\n",
        "\n",
        "    print(f\"Saved new best model to: {save_path}\")\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ground_truth_masksv))\n",
        "print(ground_truth_masksv.keys() if isinstance(ground_truth_masksv, dict) else len(ground_truth_masksv))\n"
      ],
      "metadata": {
        "id": "dvzbeYATCd7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mixgUB8H-sTs"
      },
      "source": [
        "#Step 3: Testing fine-tuned SAM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzfJwH6E-tHr"
      },
      "outputs": [],
      "source": [
        "# Set the paths to your test images and labels\n",
        "test_image_dir = \"/path/to/test/images\"   # <-- Update this path\n",
        "test_label_dir = \"/path/to/test/labels\"   # <-- Update this path\n",
        "\n",
        "# === Load and Sort Test Image Paths ===\n",
        "# Collect all test image files (e.g., .jpg)\n",
        "all_test_image_paths = sorted(glob(os.path.join(test_image_dir, \"*.jpg\")))\n",
        "test_total_images = len(all_test_image_paths)\n",
        "print(f\"Total Number of Test Images: {test_total_images}\")\n",
        "\n",
        "# === Load and Sort Test Label Paths ===\n",
        "# Collect all test label files (e.g., .png masks)\n",
        "all_test_label_paths = sorted(glob(os.path.join(test_label_dir, \"*.png\")))\n",
        "test_total_labels = len(all_test_label_paths)\n",
        "print(f\"Total Number of Test Labels: {test_total_labels}\")\n",
        "\n",
        "# === Match Image and Label Paths ===\n",
        "# These lists can now be used for DataLoader or evaluation\n",
        "Test_image_paths = all_test_image_paths[:test_total_images]\n",
        "Test_label_paths = all_test_label_paths[:test_total_labels]\n",
        "\n",
        "# Optional: Print a few samples to verify\n",
        "print(\"Sample test image path:\", Test_image_paths[0] if Test_image_paths else \"No images found\")\n",
        "print(\"Sample test label path:\", Test_label_paths[0] if Test_label_paths else \"No labels found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1vSEi8M_BHV"
      },
      "source": [
        "Box for Test *data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJZ7fH_m_Mc4"
      },
      "source": [
        "Ground_truth_test_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbvYphfc_IC9"
      },
      "outputs": [],
      "source": [
        "# Dictionary to hold ground truth binary masks for test data\n",
        "ground_truth_test_masks = {}\n",
        "\n",
        "# === Load and Process Each Test Mask ===\n",
        "for idx in range(len(Test_label_paths)):\n",
        "    # Read label image in color (3-channel); expected mask is in the red channel\n",
        "    gt_color = cv2.imread(Test_label_paths[idx])\n",
        "\n",
        "    # Extract the red channel only and convert to binary mask\n",
        "    # Note: OpenCV loads in BGR, so red is at index 2\n",
        "    binary_mask = (gt_color[:, :, 2] > 0).astype(np.float32)\n",
        "\n",
        "    # Resize if specified\n",
        "    if desired_size is not None:\n",
        "        binary_mask = cv2.resize(binary_mask, desired_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Store in dictionary\n",
        "    ground_truth_test_masks[idx] = binary_mask\n",
        "\n",
        "print(f\"Loaded {len(ground_truth_test_masks)} ground truth test masks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwtDqj8z_hUa"
      },
      "source": [
        "Load the trained model with best parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o5BRibjAII3"
      },
      "source": [
        "Prediction using Fine_tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjw_sGIk_05Z"
      },
      "outputs": [],
      "source": [
        "# === Inference with SAM Predictor on Test Set ===\n",
        "masks_tuned_list = {}   # Stores predicted binary masks\n",
        "images_tuned_list = {}  # Stores input images used during inference\n",
        "\n",
        "for idx in range(len(Test_image_paths)):\n",
        "    # === Load and Preprocess Image ===\n",
        "    image = cv2.imread(Test_image_paths[idx])\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if desired_size is not None:\n",
        "        image_rgb = cv2.resize(image_rgb, desired_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # === Set the image for SAM predictor ===\n",
        "    predictor_tuned.set_image(image_rgb)\n",
        "\n",
        "    # === Predict segmentation mask ===\n",
        "    masks_tuned, _, _ = predictor_tuned.predict(\n",
        "        point_coords=None,\n",
        "        box=None,\n",
        "        multimask_output=False,  # Only get the most confident mask\n",
        "    )\n",
        "\n",
        "    # === Extract and post-process the first predicted mask ===\n",
        "    mask_np = masks_tuned[0, :, :]                 # Select first mask\n",
        "    binary_mask = (mask_np > 0).astype(np.float32) # Convert to float binary mask\n",
        "\n",
        "    # === Store results ===\n",
        "    images_tuned_list[idx] = image_rgb\n",
        "    masks_tuned_list[idx] = binary_mask\n",
        "\n",
        "print(f\"Inference complete on {len(Test_image_paths)} test images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJS2gydcAKvB"
      },
      "source": [
        "Plot results on all of the Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdIvmRZ7_-Yc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# === Grid Configuration ===\n",
        "n_images = len(images_tuned_list)\n",
        "n_cols = 4  # Number of images per row\n",
        "n_rows = (n_images // n_cols) + (n_images % n_cols > 0)  # Auto-calculate rows\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
        "\n",
        "# If axs is 1D (e.g., only 1 row), convert to 2D for consistency\n",
        "axs = np.atleast_2d(axs)\n",
        "\n",
        "# === Iterate and Plot ===\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        index = i * n_cols + j\n",
        "        ax = axs[i, j]\n",
        "\n",
        "        if index < n_images:\n",
        "            # Display the RGB image\n",
        "            ax.imshow(images_tuned_list[index], interpolation='none')\n",
        "\n",
        "            # Generate a blue mask overlay (R=0, G=0, B=1) for binary mask = 1\n",
        "            mask = masks_tuned_list[index]\n",
        "            blue_mask_rgb = np.zeros((*mask.shape, 3), dtype=np.float32)\n",
        "            blue_mask_rgb[..., 2] = mask  # Blue channel\n",
        "\n",
        "            # Overlay the mask with transparency\n",
        "            ax.imshow(blue_mask_rgb, alpha=0.5)\n",
        "\n",
        "        # Remove axes ticks\n",
        "        ax.axis('off')\n",
        "\n",
        "# === Final Layout ===\n",
        "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo2DTq0m3m9D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# === Binary Metrics for One Prediction ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    \"\"\"\n",
        "    Computes binary segmentation metrics for a single predicted mask vs ground truth.\n",
        "    Inputs:\n",
        "        predictions (numpy array): predicted mask, float32, range [0,1] or binary\n",
        "        targets (numpy array): ground truth mask, binary (0 or 1)\n",
        "    Returns:\n",
        "        Tuple of metrics: accuracy, precision, recall, F1-score, IoU, kappa, FP, FN, TP, TN, dice\n",
        "    \"\"\"\n",
        "    # Flatten and convert to binary\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    # Confusion matrix components\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    # Metrics with small epsilon to avoid division by zero\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    # Cohen’s kappa\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Average Metrics Across Dataset ===\n",
        "def calculate_average_metrics(predictions_list, targets_list):\n",
        "    \"\"\"\n",
        "    Computes average binary segmentation metrics across a dataset.\n",
        "    Inputs:\n",
        "        predictions_list: dictionary or list of predicted masks\n",
        "        targets_list: dictionary or list of ground truth masks\n",
        "    Returns:\n",
        "        Dictionary of averaged metrics\n",
        "    \"\"\"\n",
        "    num_masks = len(predictions_list)\n",
        "\n",
        "    total_metrics = {\n",
        "        'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f_score': 0.0,\n",
        "        'iou': 0.0, 'kappa': 0.0, 'FP': 0, 'FN': 0, 'MAR': 0.0, 'FAR': 0.0, 'dice': 0.0\n",
        "    }\n",
        "\n",
        "    for i in range(num_masks):\n",
        "        pred = predictions_list[i]\n",
        "        gt = targets_list[i]\n",
        "        metrics = binary_segmentation_metrics(pred, gt)\n",
        "\n",
        "        # Accumulate each metric\n",
        "        for metric_name, value in zip(total_metrics.keys(), metrics):\n",
        "            total_metrics[metric_name] += value\n",
        "\n",
        "        # Add False Negative Rate (Missed Alarm Rate, MAR) and False Alarm Rate (FAR)\n",
        "        TP, TN, FP, FN = metrics[8], metrics[9], metrics[6], metrics[7]\n",
        "        total_metrics['MAR'] += FN / (FN + TP + 1e-5)\n",
        "        total_metrics['FAR'] += FP / (FP + TN + 1e-5)\n",
        "\n",
        "    # Compute mean for each metric\n",
        "    avg_metrics = {k: v / num_masks for k, v in total_metrics.items()}\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# === Example Usage ===\n",
        "# Evaluate the SAM predictions vs. ground truth test masks\n",
        "avg_metrics = calculate_average_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Average Metrics on Test Set ===\")\n",
        "for metric_name, value in avg_metrics.items():\n",
        "    print(f\"{metric_name.upper():<8}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-jxoBhi0YA4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from skimage.color import rgb2gray\n",
        "import math\n",
        "\n",
        "# === Configuration ===\n",
        "border_color = 'black'\n",
        "border_width = 0.35\n",
        "alpha_gray = 0.5  # transparency for base grayscale image\n",
        "alpha_tp = 0.5    # transparency for True Positives (blue)\n",
        "alpha_fa = 0.6    # False Alarms (green)\n",
        "alpha_ma = 0.6    # Missed Alarms (magenta)\n",
        "\n",
        "# === Determine Grid Layout ===\n",
        "num_images = len(images_tuned_list)\n",
        "cols = 4\n",
        "rows = math.ceil(num_images / cols)\n",
        "\n",
        "# === Setup Plot ===\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
        "axs = axs.flatten()  # flatten 2D axes array for 1D indexing\n",
        "\n",
        "# === Choose Which Images to Display (all by default) ===\n",
        "selected_indices = list(range(num_images))\n",
        "\n",
        "# === Visualization Loop ===\n",
        "for i, index in enumerate(selected_indices):\n",
        "    if index >= num_images:\n",
        "        break\n",
        "\n",
        "    # Convert image to grayscale\n",
        "    gray_image = rgb2gray(images_tuned_list[index])\n",
        "\n",
        "    # Get predicted and ground truth masks\n",
        "    pred_mask = masks_tuned_list[index]\n",
        "    gt_mask = ground_truth_test_masks[index]\n",
        "\n",
        "    ax = axs[i]\n",
        "    ax.imshow(gray_image, cmap='gray', interpolation='none', alpha=alpha_gray)\n",
        "\n",
        "    # === Mask Overlays ===\n",
        "\n",
        "    # True Positives: prediction and ground truth both 1 (blue)\n",
        "    tp_mask = (pred_mask == 1) & (gt_mask == 1)\n",
        "    tp_rgba = np.stack([np.zeros_like(tp_mask), np.zeros_like(tp_mask), tp_mask], axis=-1)\n",
        "    tp_rgba = np.concatenate([tp_rgba, tp_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(tp_rgba, alpha=alpha_tp)\n",
        "\n",
        "    # False Alarms: prediction is 1, ground truth is 0 (green)\n",
        "    fa_mask = (pred_mask == 1) & (gt_mask == 0)\n",
        "    fa_rgba = np.stack([np.zeros_like(fa_mask), fa_mask, np.zeros_like(fa_mask)], axis=-1)\n",
        "    fa_rgba = np.concatenate([fa_rgba, fa_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(fa_rgba, alpha=alpha_fa)\n",
        "\n",
        "    # Missed Alarms: prediction is 0, ground truth is 1 (magenta)\n",
        "    ma_mask = (pred_mask == 0) & (gt_mask == 1)\n",
        "    ma_rgba = np.stack([ma_mask, np.zeros_like(ma_mask), ma_mask], axis=-1)\n",
        "    ma_rgba = np.concatenate([ma_rgba, ma_mask[..., None].astype(float)], axis=-1)\n",
        "    ax.imshow(ma_rgba, alpha=alpha_ma)\n",
        "\n",
        "    # Add black border\n",
        "    ax.add_patch(Rectangle((0, 0), gray_image.shape[1], gray_image.shape[0],\n",
        "                           linewidth=border_width, edgecolor=border_color, facecolor='none'))\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "# === Turn Off Any Unused Axes ===\n",
        "for j in range(len(selected_indices), len(axs)):\n",
        "    axs[j].axis('off')\n",
        "\n",
        "# === Save and Show ===\n",
        "plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
        "output_path = \"./ViTSAM_Evaluation_Overlay.png\"  # <-- Update path as needed\n",
        "plt.savefig(output_path, dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Visualization saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcx89MdyAaAv"
      },
      "source": [
        "Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbLKpkbrYt8M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# === Metric Computation Function ===\n",
        "def binary_segmentation_metrics(predictions, targets):\n",
        "    predictions = predictions.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "    predictions_binary = (predictions > 0.5).astype(int)\n",
        "    targets_binary = targets.astype(int)\n",
        "\n",
        "    TP = np.sum((predictions_binary == 1) & (targets_binary == 1))\n",
        "    FP = np.sum((predictions_binary == 1) & (targets_binary == 0))\n",
        "    FN = np.sum((predictions_binary == 0) & (targets_binary == 1))\n",
        "    TN = np.sum((predictions_binary == 0) & (targets_binary == 0))\n",
        "\n",
        "    eps = 1e-5\n",
        "    accuracy = (TP + TN + eps) / (TP + FP + FN + TN + eps)\n",
        "    precision = (TP + eps) / (TP + FP + eps)\n",
        "    recall = (TP + eps) / (TP + FN + eps)\n",
        "    f_score = 2 * (precision * recall) / (precision + recall + eps)\n",
        "    dice = (2 * TP + eps) / (2 * TP + FP + FN + eps)\n",
        "    iou = (TP + eps) / (TP + FP + FN + eps)\n",
        "\n",
        "    total = TP + FP + FN + TN\n",
        "    p_o = (TP + TN) / total\n",
        "    p_e = ((TP + FP) * (TP + FN) + (FN + TN) * (FP + TN)) / (total ** 2)\n",
        "    kappa = (p_o - p_e) / (1 - p_e + eps)\n",
        "\n",
        "    return accuracy, precision, recall, f_score, iou, kappa, FP, FN, TP, TN, dice\n",
        "\n",
        "# === Categorization Function ===\n",
        "def categorize_metric(value, metric):\n",
        "    categories = {\n",
        "        'IoU': [(0.90, 'Excellent'), (0.85, 'Good'), (0.75, 'Fair'), (0.65, 'Poor'), (0, 'Unacceptable')],\n",
        "        'Precision': [(0.95, 'Excellent'), (0.85, 'Good'), (0.65, 'Moderate'), (0, 'Fail')],\n",
        "        'Kappa': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')],\n",
        "        'F-Score': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')],\n",
        "        'Recall': [(0.88, 'Excellent'), (0.78, 'Good'), (0.68, 'Moderate'), (0, 'Fail')]\n",
        "    }\n",
        "\n",
        "    if metric in categories:\n",
        "        for threshold, label in categories[metric]:\n",
        "            if value >= threshold:\n",
        "                return label\n",
        "    return \"Unknown\"\n",
        "\n",
        "# === Compute Metrics for All Images ===\n",
        "def compute_all_metrics(predictions_list, targets_list):\n",
        "    all_metrics = []\n",
        "    for i in range(len(predictions_list)):\n",
        "        metrics = binary_segmentation_metrics(predictions_list[i], targets_list[i])\n",
        "        metric_names = ['Accuracy', 'Precision', 'Recall', 'F-Score', 'IoU', 'Kappa', 'FP', 'FN', 'TP', 'TN', 'Dice']\n",
        "        metric_dict = dict(zip(metric_names, metrics))\n",
        "\n",
        "        # Add categorized versions\n",
        "        for name in ['IoU', 'Precision', 'Kappa', 'F-Score', 'Recall']:\n",
        "            metric_dict[f'{name}_Category'] = categorize_metric(metric_dict[name], name)\n",
        "\n",
        "        all_metrics.append(metric_dict)\n",
        "    return all_metrics\n",
        "\n",
        "# === Summarize Counts and Percentages ===\n",
        "def summarize_category_counts(metrics_list, metric_name, labels):\n",
        "    counts = {label: 0 for label in labels}\n",
        "    total = len(metrics_list)\n",
        "\n",
        "    for m in metrics_list:\n",
        "        category = m.get(f\"{metric_name}_Category\")\n",
        "        if category in counts:\n",
        "            counts[category] += 1\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Counts and Percentages of Images in Each Category for {metric_name}:\")\n",
        "    for label in labels:\n",
        "        count = counts[label]\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"{label}: {count} ({percentage:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "# === Run All ===\n",
        "metrics_list = compute_all_metrics(masks_tuned_list, ground_truth_test_masks)\n",
        "\n",
        "summarize_category_counts(metrics_list, 'IoU',        ['Excellent', 'Good', 'Fair', 'Poor', 'Unacceptable'])\n",
        "summarize_category_counts(metrics_list, 'Precision',  ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'Kappa',      ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'F-Score',    ['Excellent', 'Good', 'Moderate', 'Fail'])\n",
        "summarize_category_counts(metrics_list, 'Recall',     ['Excellent', 'Good', 'Moderate', 'Fail'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}